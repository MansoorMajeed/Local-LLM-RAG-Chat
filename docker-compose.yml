
services:
  chroma:
    image: chromadb/chroma:latest
    ports:
      - "8000:8000"
    volumes:
      - chroma_data:/data
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_HTTP_PORT=8000
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/api/v2/heartbeat')"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

  processor:
    build: 
      context: ./processor
      dockerfile: Dockerfile
    depends_on:
      - chroma
    volumes:
      - ${DOCUMENTS_PATH:-./documents}:/app/documents:ro
      - ./processor:/app
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-host.docker.internal:11434}
      - CHROMA_HOST=${CHROMA_HOST:-chroma:8000}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-mxbai-embed-large}
      - DOCUMENTS_PATH=/app/documents
      - BATCH_SIZE=${BATCH_SIZE:-10}
      - OLLAMA_RETRIES=${OLLAMA_RETRIES:-3}
      - POSTHOG_DISABLED=true
      - UNSTRUCTURED_DISABLE_TELEMETRY=true
      - ANONYMIZED_TELEMETRY=False
    ports:
      - "8001:8001"
    extra_hosts:
      - "host-gateway:host-gateway"
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'

  mcp-server:
    build:
      context: ./mcp-server
      dockerfile: Dockerfile
    depends_on:
      - chroma
    volumes:
      - ./mcp-server:/app
    environment:
      - CHROMA_HOST=${CHROMA_HOST:-chroma:8000}
      - OLLAMA_HOST=${OLLAMA_HOST:-host.docker.internal:11434}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-mxbai-embed-large}
    ports:
      - "8002:8002"
    extra_hosts:
      - "host-gateway:host-gateway"
    stdin_open: true
    tty: true
    command: ["python", "/app/server_fastmcp.py"]
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

  rag-api:
    build:
      context: ./mcp-server
      dockerfile: Dockerfile
    depends_on:
      - chroma
    volumes:
      - ./mcp-server:/app
    environment:
      - CHROMA_HOST=${CHROMA_HOST:-chroma:8000}
      - OLLAMA_HOST=${OLLAMA_HOST:-host.docker.internal:11434}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-mxbai-embed-large}
      - HTTP_PORT=8003
    ports:
      - "8003:8003"
    extra_hosts:
      - "host-gateway:host-gateway"
    command: ["python", "/app/http_server.py"]
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    depends_on:
      - rag-api
    volumes:
      - open_webui_data:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_HOST:-http://host.docker.internal:11434}
      - WEBUI_NAME="Local RAG Chat"
      - ENABLE_SIGNUP=${ENABLE_SIGNUP:-true}
      - DEFAULT_USER_ROLE=${DEFAULT_USER_ROLE:-user}
    ports:
      - "8080:8080"
    extra_hosts:
      - "host-gateway:host-gateway"
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'

  chat-interface:
    build:
      context: ./chat-interface
      dockerfile: Dockerfile
    depends_on:
      - processor
    volumes:
      - ./chat-interface:/app
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-host.docker.internal:11434}
      - PROCESSOR_HOST=${PROCESSOR_HOST:-processor:8001}
      - CHAT_MODEL=${CHAT_MODEL:-llama3.1:8b}
    ports:
      - "8005:8003"
    extra_hosts:
      - "host-gateway:host-gateway"
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
    profiles:
      - legacy

volumes:
  chroma_data:
  open_webui_data:
